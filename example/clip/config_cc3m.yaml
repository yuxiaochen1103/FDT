model:
    type: clip_vitb32
    kwargs:
        image_encode:
            embed_dim: 512
        text_encode:
            bpe_path: 'text_info/bpe_simple_vocab_16e6.txt.gz'
            text_encode_type: Transformer #Transformer,Bert,GPT2
            text_model_utils:
                random: False
                freeze: False
            embed_dim: 512
        clip:
            use_allgather: True

grad_clip:
    type: logit_scale_param_value
    value: 3
    max_value: 6


optimizer:
    type: AdamW
    kwargs:
        lr: 0.0001  # 5e-4
        weight_decay: 0.1
        betas: [0.9, 0.98]
        amsgrad: False
        eps: 0.00000001

    pconfig:
        bn_w:
            weight_decay: 0
        bn_b:
            weight_decay: 0
        ln_w:
            weight_decay: 0
        ln_b:
            weight_decay: 0
        bias:
            weight_decay: 0
        logit_scale:
            # lr: 0.0001  # not useful
            weight_decay: 0


lr_scheduler:
    type: Cosine
    kwargs:
        base_lr: 0.0001
        warmup_lr: 0.001
        min_lr: 0.0
        warmup_steps: 2500
        max_iter: 86560 # 2705 * 32


data:
    train:
        epoch: 32
        data_path: /research/cbim/vast/yc984/img_txt_dataset/cc3m/cc3m_2/{00000..02769}.tar # default, other choices: _256, _512, _1024, _2048
        transforms: MOCOV2_single
        num_samples: 2769025
        num_shards: 2770
        workers: 5
        batch_size: 128

    test:
        data_fold: /research/cbim/archive/yc984/img_dataset


saver:
    print_freq: 100
    val_freq: 2700
    save_freq: 2700 #each epoch has 2705 iteration
    save_many: True